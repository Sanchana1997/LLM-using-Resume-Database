{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4QZmt65Xovo"
      },
      "source": [
        "# **Title : LLM Training Phase 1**\n",
        "# **Document by : Sanchana Mohankumar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sHwBlvwXupn"
      },
      "source": [
        "### **Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfBLLCX94DeE",
        "outputId": "69abb29b-04a4-47b6-df00-05da9914eb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data**\n",
        "\n",
        "Link - https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n"
      ],
      "metadata": {
        "id": "KE57moeAsWsQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzTENH1ZX1_R"
      },
      "source": [
        "## **Installations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACwnLhsn5QCX"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6icTC1Fx5hRC"
      },
      "outputs": [],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "Resp-HZuLPsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L1-71U5V68nU"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import json\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "\n",
        "import PyPDF2\n",
        "import openai\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API KEY**"
      ],
      "metadata": {
        "id": "Xi6dJuZDLtuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI API key\n",
        "openai.api_key = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "pHFLYJcFLl0Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing Downloads**\n",
        "\n"
      ],
      "metadata": {
        "id": "i3QHJ8qzLwtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Initialized NLTK's stopwords\n",
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "JDv1X0I5Ljiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhpBzBbaW4V"
      },
      "source": [
        "# **PART 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TASK 1:**\n",
        "Task 1 code automates the process of extracting text from a PDF resume, sends it to GPT-3 for further processing, receives a JSON-like response, and stores it in a specified file on Google Drive. In this task we are passing 1 file and checking the output json file.\n",
        "\n",
        "\n",
        "This code performs the following tasks:\n",
        "\n",
        "**1. Text Extraction:** It extracts text content from a PDF file located at pdf_resume_path using the PyPDF2 library. This extracted text is essentially the content of a resume.\n",
        "\n",
        "**2. Prompt Construction:** It constructs a GPT-3 prompt by including the extracted resume text within a structured prompt. The prompt asks GPT-3 to generate a JSON-like output based on the resume content.\n",
        "\n",
        "**3. GPT-3 Interaction:** It interacts with the OpenAI GPT-3 API, using the constructed prompt. It requests GPT-3 to generate a text response in JSON format based on the provided resume.\n",
        "\n",
        "**4. JSON Output Retrieval:** The generated JSON-like output from GPT-3 is obtained and stored in the raw_output_text variable.\n",
        "\n",
        "**5. File Saving:** The JSON output is saved to a file located at output_file_path, which is specified as \"/content/drive/MyDrive/output_1.json\". This allows you to store the generated JSON output in your Google Drive.\n",
        "\n",
        "**6. Printing:** Finally, the generated JSON-like output is printed to the console.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Ct12yg-LpFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converted to text from PDF file using PyPDF2 library\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "# Constructed and sent the prompt to GPT-3\n",
        "def generate_raw_output(pdf_resume_path):\n",
        "    resume_text = extract_text_from_pdf(pdf_resume_path)\n",
        "\n",
        "    prompt = f\"Given the following resume:\\n{resume_text}\\n\\nPlease generate a JSON output.\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # Max Tokens Allowed 4097\n",
        "        prompt=prompt,\n",
        "        max_tokens= 2500\n",
        "    )\n",
        "\n",
        "    output_text = response.choices[0].text.strip()\n",
        "    return output_text\n",
        "\n",
        "# Provided the path to the PDF resume file\n",
        "pdf_resume_path = \"/content/drive/MyDrive/Resume_Database/Sanchana_Mohankumar_Resume.pdf\"\n",
        "\n",
        "# Generated raw JSON-like output using GPT-3\n",
        "raw_output_text = generate_raw_output(pdf_resume_path)\n",
        "\n",
        "# Saved the JSON output to a file in Google Drive\n",
        "output_file_path = \"/content/drive/MyDrive/output_1.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    output_file.write(raw_output_text)\n",
        "\n",
        "print(raw_output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmaReTXiOobr",
        "outputId": "f4553fac-cdc8-4931-b21a-9d57569b352a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"name\": \"Sanchana\",\n",
            "  \"contactInfo\": {\n",
            "    \"phoneNumber\": \"+1 (206) 209-9901\",\n",
            "    \"email\": \"mohankumar.s@northeastern.edu\",\n",
            "    \"name\": \"Mohankumar\"\n",
            "  },\n",
            "  \"socialProfiles\": {\n",
            "    \"linkedin\": \"www.linkedin.com/in/sanchanamohankumar\",\n",
            "    \"github\": \"https://github.com/Sanchana1997\"\n",
            "  },\n",
            "  \"backgroundSummary\": \"Experienced and driven Data Analyst with almost 2 years of experience in delivering results. Seeking new challenges and opportunities in field of Data to apply my skills and expertise.\",\n",
            "  \"workExperience\": [\n",
            "    {\n",
            "      \"position\": \"Graduate Teaching Assistant\",\n",
            "      \"employer\": \"Northeastern University\",\n",
            "      \"timePeriod\": \"January 2023 - April 2023\",\n",
            "      \"jobsPerformed\": [\n",
            "        \"Demonstrated technical expertise in python, statistical analysis, and machine learning while conducting lab sessions, grading assignments, and assisting students with data mining course work.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"position\": \"Data Analyst\",\n",
            "      \"employer\": \"Accenture\",\n",
            "      \"timePeriod\": \"June 2019 - January 2021\",\n",
            "      \"jobsPerformed\": [\n",
            "      \"Extracted and transformed data from MSSQL server with SAP data services ETL tool using SQL queries to load data into SAP software for inventory and production purposes.\",\n",
            "      \"Applied technical skills using SQL, Python in data analysis and reporting to provide solutions to client requests.\",\n",
            "      \"Collaborated with 20-member cross-functional (sales, warehouse, material) team and coordinated with 3rd party clients toward the launch of SAP ERP system across 3 countries simultaneously.\",\n",
            "      \"Applied intermediate Excel skills in analyzing and exploring data for testing ERP system test scenario.\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"education\": [\n",
            "    {\n",
            "      \"degree\": \"Master of Science, Data Analytics Engineering\",\n",
            "      \"university\": \"Northeastern University, WA, USA\",\n",
            "      \"graduationDate\": \"April 2023\",\n",
            "      \"relatedCourses\": [\n",
            "        \"Data Visualization\",\n",
            "        \"Big Data\",\n",
            "        \"Machine Learning\",\n",
            "        \"NLP\",\n",
            "        \"Database Management\",\n",
            "        \"Algorithms\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"degree\": \"B. Tech, Electronics and Communication Engineering\",\n",
            "      \"university\": \"Amrita University, TN, India\",\n",
            "      \"graduationDate\": \"May 2019\"\n",
            "    }\n",
            "  ],\n",
            "  \"skills\": {\n",
            "    \"languagesAndDatabases\": [\n",
            "      \"SQL\",\n",
            "      \"Python\",\n",
            "      \"R\",\n",
            "      \"MongoDB\",\n",
            "      \"MS SQL Server\"\n",
            "    ],\n",
            "    \"toolsAndSoftware\": [\n",
            "      \"Tableau\",\n",
            "      \"Power BI\",\n",
            "      \"MS Excel\",\n",
            "      \"AWS S3\",\n",
            "      \"AWS EMR\",\n",
            "      \"Hive\",\n",
            "      \"Hadoop\",\n",
            "      \"Pig\",\n",
            "      \"Lucidchart\"\n",
            "    ],\n",
            "    \"libraries\": [\n",
            "      \"Pandas\",\n",
            "      \"NumPy\",\n",
            "      \"Scikit-learn\",\n",
            "      \"Matplotlib\",\n",
            "      \"ggplot2\",\n",
            "      \"tidyverse\",\n",
            "      \"dplyr\",\n",
            "      \"keras\",\n",
            "      \"Shiny\",\n",
            "      \"plotly\"\n",
            "    ],\n",
            "    \"dataProcessingAndAlgorithm\": [\n",
            "      \"Text Analysis\",\n",
            "      \"Regression\",\n",
            "      \"Classification\",\n",
            "      \"Clustering\"\n",
            "    ]\n",
            "  },\n",
            "  \"projects\": [\n",
            "    {\n",
            "      \"title\": \"Weather Forecast | Big Data Parallel Processing using AWS | SELF PROJECT\",\n",
            "      \"description\": [\n",
            "        \"Designed and executed an efficient ETL process using AWS Glue to transform and load temperature data for use in ARIMA models for weather forecasting.\",\n",
            "        \"Leveraged AWS S3 for secure and scalable storage of temperature data and model outputs, ensuring easy accessibility and efficient data management.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Covid19 Fake News Detection | Natural Language Processing using Python | SELF PROJECT\",\n",
            "      \"description\": [\n",
            "        \"Detected Covid19 fake news deploying NLP model by analyzing ~10k Covid19 tweets posted during pandemic.\",\n",
            "        \"Leveraged tf-idf, fastText, and GloVe word embedding techniques to implement SVM, Logistic Regression, BI-LSTM, and BERT transformer model with final accuracy of 97%.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Meal Kit Delivery Database | Database Design and Management using SQL\",\n",
            "      \"description\": [\n",
            "        \"Build a MySQL meal kit delivery database system using 3rd Normal form modelling technique with views, stored procedure, constraints, and computed columns to provide seamless end to end meal kit experience for users.\",\n",
            "        \"Created Tableau Dashboards to execute user geographical concentration trend analysis further expand meal kit delivery geographically.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"title\": \"Global Issues | Data Visualization using R\",\n",
            "      \"description\": [\n",
            "        \"Extracted 10+ Global Issue datasets containing over 1 million data to analyze trends and patterns on major global issues such as poverty, and food security through interactive graphs leveraging RShiny webpage.\",\n",
            "        \"Performed data wrangling and data cleaning techniques on raw data set to retrieve informative data for data analysis.\"\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"publications\": [\n",
            "    {\n",
            "      \"title\": \"A wearable device to detect blood volume changes\",\n",
            "      \"link\": \"https://ieeexplore.ieee.org/document/8728520\",\n",
            "      \"description\": [\n",
            "        \"Designed and developed a wearable device to detect changes in blood volume using Pulse Rate Variability (PRV) as a response to the increase in cardiovascular cases in the medical field.\",\n",
            "        \"Leveraged MySQL database to store the data gathered from human subjects and used an internet web host platform to securely access and manipulate the data.\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TASK 2:**"
      ],
      "metadata": {
        "id": "VtTj_W0SOrbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **tf-idf Vectorization**\n",
        "In Task 2, I automated the process of extracting text from PDF resumes, sending it to GPT-3 for further processing, receiving a JSON-like response as we did in Task 1 further proceding with vectorizing it using TF-IDF, calculating cosine similarity scores with an input resume, and identifying and printing the top 5 most similar resumes along with their similarity scores.\n",
        "\n",
        "**1.Text Extraction:** It defines a function extract_text_from_pdf to extract text content from a PDF file using the PyPDF2 library. The extracted text represents the content of a resume.\n",
        "\n",
        "**2.GPT-3 Prompt Construction:** It defines a function generate_raw_output that extracts text from a specified PDF resume file, constructs a GPT-3 prompt by embedding the extracted text, and sends this prompt to the GPT-3 model. The model is instructed to generate a JSON-like output based on the provided resume text.\n",
        "\n",
        "**3.TF-IDF Vectorization:** It defines a function vectorize_resume to convert resume text into a TF-IDF vector using scikit-learn's TF-IDF vectorizer.\n",
        "\n",
        "**4.Main Function:** In the main function:\n",
        "\n",
        "- It loads and vectorizes an input resume (specified by input_resume_path) using TF-IDF.\n",
        "- Creates a TF-IDF vectorizer and fits it with the input resume text.\n",
        "- Vectorizes all resumes in the database folder using TF-IDF and stores them as TF-IDF matrices.\n",
        "- Calculates the cosine similarity between the input resume and all resumes in the database using TF-IDF vectors.\n",
        "- Retrieves the indices of the most similar resumes based on cosine similarity and their respective similarity scores.\n",
        "- Prints the top 5 most similar resume filenames along with their cosine similarity scores."
      ],
      "metadata": {
        "id": "B-HtV3j1P-Fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a7de00-16b0-40d4-d8a4-8d57f7accef2",
        "id": "3MfZw4mgQtOB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most similar resume filenames and their cosine similarity scores:\n",
            "Filename: Data_Resume.pdf, Similarity Score: 1.0\n",
            "Filename: Sanchana_Mohankumar_Resume.pdf, Similarity Score: 0.8420834224000379\n",
            "Filename: Data_Scientist_Resume.pdf, Similarity Score: 0.7429042776944733\n",
            "Filename: Data2_Resume.pdf, Similarity Score: 0.718642800041404\n",
            "Filename: Fitness_Resume.pdf, Similarity Score: 0.7138810369392904\n"
          ]
        }
      ],
      "source": [
        "# Defined a function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "# Constructed and send the prompt to GPT-3\n",
        "def generate_raw_output(pdf_resume_path):\n",
        "    resume_text = extract_text_from_pdf(pdf_resume_path)\n",
        "\n",
        "    prompt = f\"Given the following resume:\\n{resume_text}\\n\\nPlease generate a JSON output.\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=2500\n",
        "    )\n",
        "\n",
        "    text = response.choices[0].text.strip()\n",
        "    return text\n",
        "\n",
        "# Defined a function to vectorize the text using TF-IDF\n",
        "def vectorize_resume(text, tfidf_vectorizer):\n",
        "    tfidf_matrix = tfidf_vectorizer.transform([text])\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Loaded and vectorized the input resume using TF-IDF\n",
        "    input_resume_path = \"/content/drive/MyDrive/Resume_Database/Data_Resume.pdf\"\n",
        "    input_resume_text = extract_text_from_pdf(input_resume_path)\n",
        "\n",
        "    # Created a TF-IDF vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_vectorizer.fit([input_resume_text])\n",
        "\n",
        "    # Vectorized the resumes in the database using TF-IDF\n",
        "    database_resumes = []\n",
        "    database_folder = \"/content/drive/MyDrive/Resume_Database\"\n",
        "\n",
        "    # Listed all files in the Resume_Database folder\n",
        "    pdf_resume_filenames = [filename for filename in os.listdir(database_folder) if filename.lower().endswith(\".pdf\")]\n",
        "\n",
        "    for pdf_resume_filename in pdf_resume_filenames:\n",
        "        db_resume_path = os.path.join(database_folder, pdf_resume_filename)\n",
        "        db_resume_text = extract_text_from_pdf(db_resume_path)\n",
        "        db_resume_vector = vectorize_resume(db_resume_text, tfidf_vectorizer)\n",
        "        database_resumes.append(db_resume_vector)\n",
        "\n",
        "    # Converted the list of TF-IDF vectors to a numpy array for similarity calculation\n",
        "    database_matrix = [tfidf_vector.toarray()[0] for tfidf_vector in database_resumes]\n",
        "\n",
        "    # Calculated the cosine similarity between the input resume and all resumes in the database\n",
        "    input_resume_vector = tfidf_vectorizer.transform([input_resume_text])\n",
        "    cosine_similarities = cosine_similarity(input_resume_vector, database_matrix).flatten()\n",
        "\n",
        "    # Got the indices of the most similar resumes and their cosine similarity scores\n",
        "    indices = cosine_similarities.argsort()[::-1]\n",
        "    top_5_similar_resumes = [(pdf_resume_filenames[idx], cosine_similarities[idx]) for idx in indices[:5]]\n",
        "\n",
        "    # Printed the top 5 most similar resume filenames along with their cosine similarity scores\n",
        "    print(\"Top 5 most similar resume filenames and their cosine similarity scores:\")\n",
        "    for resume_filename, similarity_score in top_5_similar_resumes:\n",
        "        print(f\"Filename: {resume_filename}, Similarity Score: {similarity_score}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Doc2Vec Vectorization**\n",
        "In Task 2, I automated the process of extracting text from PDF resumes, sending it to GPT-3 for further processing, receiving a JSON-like response as we did in Task 1 further proceding with vectorizing it using Doc2Vec, calculating cosine similarity scores with an input resume, and identifying and printing the top 5 most similar resumes along with their similarity scores.\n",
        "\n",
        "**1.Text Extraction:** It defines a function extract_text_from_pdf to extract text content from a PDF file using the PyPDF2 library. The extracted text represents the content of a resume.\n",
        "\n",
        "**2. Text Preprocessing and Tagging**: Preprocess_text is responsible for preprocessing the extracted text. It tokenizes the text by splitting it into words and removes punctuation, stopwords, and other noise. The result is a cleaned and tokenized version of the text. The text is then tagged using the Gensim library's TaggedDocument object, associating each document with its original text.\n",
        "\n",
        "**3.GPT-3 Prompt Construction:** It defines a function generate_raw_output that extracts text from a specified PDF resume file, constructs a GPT-3 prompt by embedding the extracted text, and sends this prompt to the GPT-3 model. The model is instructed to generate a JSON-like output based on the provided resume text.\n",
        "\n",
        "**4. Main Function:** The main function is where everything comes together:\n",
        "\n",
        "- Load and Preprocess Data: It loads and preprocesses the input resume text and the text of resumes in the database. The input resume and database resumes are represented as TaggedDocument objects.\n",
        "- Model Training: It trains a Doc2Vec model using the TaggedDocuments from the input and database resumes. The model learns vector representations of the resume texts.\n",
        "- Cosine Similarity Calculation: It calculates cosine similarity scores between the input resume and all resumes in the database. The similarity scores indicate how closely related each database resume is to the input resume.\n",
        "- Identify Top Matches: It identifies the top 5 resumes in the database that have the highest cosine similarity scores with the input resume.\n",
        "- Display Results: It displays the filenames of the top 5 most similar resumes along with their respective similarity scores.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vQdTgPZYVOyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to preprocess text and create TaggedDocument objects\n",
        "def preprocess_text(text):\n",
        "    # Split text into words and remove punctuation, stopwords, etc.\n",
        "    words = gensim.utils.simple_preprocess(text)\n",
        "    return TaggedDocument(words=words, tags=[text])\n",
        "\n",
        "# Function to generate a JSON output using GPT-3\n",
        "def generate_json_output(resume_text):\n",
        "    prompt = f\"Given the following resume:\\n{resume_text}\\n\\nPlease generate a JSON output.\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=2500\n",
        "    )\n",
        "\n",
        "    json_output = response.choices[0].text.strip()\n",
        "    return json_output\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess the input resume\n",
        "    input_resume_path = \"/content/drive/MyDrive/Resume_Database/Data_Resume.pdf\"\n",
        "    input_resume_text = extract_text_from_pdf(input_resume_path)\n",
        "    input_resume_doc = preprocess_text(input_resume_text)\n",
        "\n",
        "    # Generated JSON output using GPT-3\n",
        "    json_output = generate_json_output(input_resume_text)\n",
        "\n",
        "    # Preprocessed and tag the resumes in the database\n",
        "    database_resumes = []\n",
        "    database_folder = \"/content/drive/MyDrive/Resume_Database\"\n",
        "\n",
        "    pdf_resume_filenames = [filename for filename in os.listdir(database_folder) if filename.lower().endswith(\".pdf\")]\n",
        "\n",
        "    for pdf_resume_filename in pdf_resume_filenames:\n",
        "        db_resume_path = os.path.join(database_folder, pdf_resume_filename)\n",
        "        db_resume_text = extract_text_from_pdf(db_resume_path)\n",
        "        db_resume_doc = preprocess_text(db_resume_text)\n",
        "        database_resumes.append(db_resume_doc)\n",
        "\n",
        "    # Trained a Doc2Vec model\n",
        "    model = Doc2Vec(vector_size=300, min_count=2, epochs=40)\n",
        "    model.build_vocab([input_resume_doc] + database_resumes)\n",
        "    model.train([input_resume_doc] + database_resumes, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "    # Calculated cosine similarity scores\n",
        "    similarity_scores = []\n",
        "    input_vector = model.dv[input_resume_doc.tags[0]]\n",
        "    for db_resume in database_resumes:\n",
        "        db_vector = model.dv[db_resume.tags[0]]\n",
        "        similarity = cosine_similarity([input_vector], [db_vector])[0][0]\n",
        "        similarity_scores.append(similarity)\n",
        "\n",
        "    # Created a list of tuples with filename and similarity score\n",
        "    similarity_results = list(zip(pdf_resume_filenames, similarity_scores))\n",
        "\n",
        "    # Sorted the results by similarity score in descending order\n",
        "    similarity_results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"Top 5 Resumes with the Highest Cosine Similarity Scores:\")\n",
        "    for filename, similarity_score in similarity_results[:5]:\n",
        "        print(f\"Filename: {filename}, Similarity Score: {similarity_score:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL6Q38hsSpAF",
        "outputId": "7ce14ced-6d31-4305-8d06-853dd6742bea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Resumes with the Highest Cosine Similarity Scores:\n",
            "Filename: Data_Resume.pdf, Similarity Score: 1.0000\n",
            "Filename: Sanchana_Mohankumar_Resume.pdf, Similarity Score: 0.8374\n",
            "Filename: Data2_Resume.pdf, Similarity Score: 0.7934\n",
            "Filename: Robotics_Engineer_Resume.pdf, Similarity Score: 0.6508\n",
            "Filename: Data_Scientist_Resume.pdf, Similarity Score: 0.5340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1 Conclusion**\n",
        "\n",
        "As we can see in this phase I have used 1 resume for Task 1 to output the input resume in Json format. Further in Task 2 we have used tf-idf and Doc2vec Vectorization to see the difference in Similarity scores.\n",
        "\n",
        "- While TF-IDF has its strengths, such as simplicity and interpretability, it may not capture the distinct semantic relationships present in documents like resumes. Doc2Vec, on the other hand, is better suited for tasks that require understanding the meaning and context of text, making it a more appropriate choice for matching and ranking resumes in this case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "thDumUCiYFN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1 Future Enhancements**\n",
        "\n",
        "**1.Feature Engineering:** My efforts extended to feature engineering, where I explored various text representations. While I effectively utilized tf-idf and Doc2vec, it's important to recognize that the realm of word embeddings offers a plethora of techniques to experiment with. By delving into both established and cutting-edge methods, I can further refine the accuracy and precision of the resume matching system.\n",
        "\n",
        "**2.Text Preprocessing:** A pivotal aspect of my strategy was text preprocessing. Beyond my current techniques, there is room for implementing advanced preprocessing methods. These techniques can significantly improve the quality of input data, leading to more precise and relevant results in the resume matching process.\n",
        "\n",
        "**3.Similarity Metrics:** The foundation of my resume matching system lies in similarity metrics. While I've employed metrics like cosine similarity, exploring a wider range of metrics such as Jaccard similarity, Euclidean distance, and other innovative alternatives could bring about refinements that enhance the overall effectiveness and reliability of the system.\n"
      ],
      "metadata": {
        "id": "ydDJUySvcuD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deliverables PART 1:**\n",
        "- Task 1:\n",
        "Input_data_Task_1\n",
        "Output_task_1\n",
        "- Task 2:\n",
        "Input_data_Task_2,\n",
        "Output_task_2"
      ],
      "metadata": {
        "id": "SrOwgOQfvlca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PART 2**"
      ],
      "metadata": {
        "id": "1-HcZZruOpcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TASK 1:**\n",
        "\n",
        "This code provides a comprehensive workflow to preprocess and enhance multiple PDF resumes using GPT-3, making it suitable for various applications like resume parsing and improvement.\n",
        "\n",
        "**1. Text Extraction (`extract_text_from_pdf`):**\n",
        "- This function extracts the text content from a PDF file. It opens the PDF, reads its pages, and accumulates the text.\n",
        "\n",
        "**2. Text Preprocessing (`preprocess_text`):**\n",
        "- Text preprocessing is essential to improve the quality of input data for GPT-3. All text is converted to lowercase to ensure consistency and various other text preprocessing steps are perfomed like Tokenization, Stopword Removal, Noise Reduction and Text Normalization.\n",
        "\n",
        "**Text Enhancement (`process_chunk_with_gpt`):**\n",
        "- This function uses GPT-3 to improve text quality. It takes several inputs:\n",
        "  - `prompt`: A predefined prompt instructing GPT-3 on what to do.\n",
        "  - `chunk`: A portion of the text for GPT-3 to work on.\n",
        "  - `resume_text`: The entire resume text (context).\n",
        "  - `start_date`, `end_date`, `experience_details`: Specific details to be incorporated into the text.\n",
        "- It assembles a full prompt with the provided data and sends it to GPT-3. GPT-3 processes the prompt and returns enhanced text.\n",
        "\n",
        "**Single Resume Processing (`process_single_pdf`):**\n",
        "- This function takes a single PDF resume and enhances it\n",
        "\n",
        "**Batch Processing (`process_pdfs_in_folder`):**\n",
        "- This function handles multiple resumes efficiently using multiprocessing which enhance resumes in parallel:\n",
        "- Outputs the enhanced data in JSON format to \"output.json.\"\n"
      ],
      "metadata": {
        "id": "Y5Voki3LOpez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, \"rb\") as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to perform text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Stopword removal and noise removal\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Text normalization\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to process a chunk of text using GPT-3\n",
        "def process_chunk_with_gpt(prompt, chunk, resume_text, start_date, end_date, experience_details):\n",
        "    # Replace placeholders with actual data\n",
        "    full_prompt = f\"{prompt}\\n\\n{chunk}\"\n",
        "    full_prompt = full_prompt.replace(\"{resume_text}\", resume_text)\n",
        "    full_prompt = full_prompt.replace(\"{start_date}\", start_date)\n",
        "    full_prompt = full_prompt.replace(\"{end_date}\", end_date)\n",
        "    full_prompt = full_prompt.replace(\"{experience_details}\", experience_details)\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=full_prompt,\n",
        "        max_tokens=1500\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Function to process a single PDF file\n",
        "def process_single_pdf(pdf_path, prompt, start_date, end_date, experience_details):\n",
        "    resume_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Preprocess the text\n",
        "    cleaned_resume_text = preprocess_text(resume_text)\n",
        "\n",
        "    # Process the text in smaller chunks using GPT-3\n",
        "    chunk_size = 6000  # Adjust the chunk size as needed\n",
        "    chunked_text = [cleaned_resume_text[i:i+chunk_size] for i in range(0, len(cleaned_resume_text), chunk_size)]\n",
        "    generated_text = \"\"\n",
        "    for chunk in chunked_text:\n",
        "        # Replace placeholders with actual datai\n",
        "        generated_chunk = process_chunk_with_gpt(prompt, chunk, cleaned_resume_text, start_date, end_date, experience_details)\n",
        "        generated_text += generated_chunk\n",
        "\n",
        "    return pdf_path, generated_text\n",
        "\n",
        "# Function to process PDFs in a folder and its subdirectories using multiprocessing\n",
        "def process_pdfs_in_folder(root_folder):\n",
        "    output_data = {}  # Initializing an empty dictionary to store output data\n",
        "    pdf_files = []  # Created a list to store PDF file paths\n",
        "\n",
        "    for root, dirs, files in os.walk(root_folder):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                pdf_resume_path = os.path.join(root, file_name)\n",
        "                pdf_files.append(pdf_resume_path)  # Stored PDF file paths\n",
        "\n",
        "    # Created a pool of worker processes\n",
        "    pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n",
        "\n",
        "    # Defined placeholder values for resume_text, start_date, end_date, and experience_details\n",
        "    resume_text = \"Placeholder for resume text\"\n",
        "    start_date = \"Placeholder for start date\"\n",
        "    end_date = \"Placeholder for end date\"\n",
        "    experience_details = \"Placeholder for experience details\"\n",
        "\n",
        "    # Used the map function to process PDF files in parallel\n",
        "    results = pool.starmap(process_single_pdf, [(pdf_path, prompt, start_date, end_date, experience_details) for pdf_path in pdf_files])\n",
        "\n",
        "    # Closed the pool of worker processes\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    # Stored the results in the output dictionary\n",
        "    for pdf_path, generated_text in results:\n",
        "        output_data[os.path.basename(pdf_path)] = generated_text\n",
        "\n",
        "    # Output the data in JSON format\n",
        "    with open(\"output.json\", \"w\") as json_file:\n",
        "        json.dump(output_data, json_file, indent=4)\n",
        "\n",
        "# Provided the root folder path where subfolders contain PDF files\n",
        "root_folder = \"/content/drive/MyDrive/data/AGRICULTURE_ART_Resumes\"\n",
        "\n",
        "# Defined my prompt here\n",
        "prompt = \"Given the following resume:\\n{resume_text}\\n\\nPlease generate a JSON output.\"\n",
        "\n",
        "# Processed PDF files in the specified folder and subdirectories using multiprocessing\n",
        "process_pdfs_in_folder(root_folder)\n"
      ],
      "metadata": {
        "id": "CDouTUdfRZ36"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_m0MLlSTiB3"
      },
      "outputs": [],
      "source": [
        "# Load the JSON data from the file\n",
        "with open(\"output.json\", \"r\") as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Print the data\n",
        "print(json.dumps(data, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TASK 2:**\n",
        "\n",
        "This code is used for matching an input resume against a database of resumes based on their text content using TF-IDF and cosine similarity. It helps identify the most similar resumes in the database to the input resume.\n",
        "\n",
        "1. **Loading JSON Data:** It defines a function `load_json_data` to load JSON data from a file. The JSON data is expected to contain text data, typically from processed resumes.\n",
        "\n",
        "2. **Text Vectorization:** It defines a function `vectorize_text_data` to vectorize text data using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization.\n",
        "\n",
        "3. **Cosine Similarity Calculation:** It defines a function `calculate_cosine_similarity` to calculate the cosine similarity between an input vector and a matrix of vectors. In this context, it calculates the cosine similarity between an input resume and a database of resumes.\n",
        "\n",
        "4. **Main Function:** In the main function:\n",
        "\n",
        "   - It loads JSON data from a file (the path needs to be specified).\n",
        "   - Extracts text data from the loaded JSON data.\n",
        "   - Vectorizes the text data using TF-IDF.\n",
        "   - Loads an input resume (the path needs to be specified) and vectorizes it using the same TF-IDF vectorizer.\n",
        "   - Calculates the cosine similarity between the input resume and all resumes in the database.\n",
        "   - Retrieves the indices of the most similar resumes based on cosine similarity.\n",
        "   - Prints the top 5 most similar resume filenames along with their cosine similarity scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "2LClQG6LWioV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtzGOgrPY4h5",
        "outputId": "09796672-a372-4d01-e323-1b2cd26b81fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most similar resume filenames and their similarity scores:\n",
            "Filename: 10953078.pdf, Similarity Score: 0.8873\n",
            "Filename: 24061629.pdf, Similarity Score: 0.3975\n",
            "Filename: 36102323.pdf, Similarity Score: 0.3599\n",
            "Filename: 17694454.pdf, Similarity Score: 0.3582\n",
            "Filename: 46055835.pdf, Similarity Score: 0.3452\n"
          ]
        }
      ],
      "source": [
        "# Function to load JSON data from a file\n",
        "def load_json_data(file_path):\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "        data = json.load(json_file)\n",
        "    return data\n",
        "\n",
        "# Function to vectorize text data using TF-IDF\n",
        "def vectorize_text_data(text_data):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
        "    return tfidf_matrix, tfidf_vectorizer\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(input_vector, database_matrix):\n",
        "    cosine_similarities = cosine_similarity(input_vector, database_matrix)\n",
        "    return cosine_similarities\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load the JSON data from the file\n",
        "    json_file_path = \"/content/output.json\"  # Replace with the correct path to your JSON file\n",
        "    json_data = load_json_data(json_file_path)\n",
        "\n",
        "    # Extracted the text data from the JSON file\n",
        "    text_data = list(json_data.values())\n",
        "\n",
        "    # Vectorized the text data using TF-IDF\n",
        "    tfidf_matrix, tfidf_vectorizer = vectorize_text_data(text_data)\n",
        "\n",
        "    # Defined the input resume text (you can change this)\n",
        "    input_resume_path = \"/content/drive/MyDrive/data/AGRICULTURE_ART_Resumes/AGRICULTURE/10953078.pdf\"\n",
        "    input_resume_text = extract_text_from_pdf(input_resume_path)\n",
        "\n",
        "    # Vectorized the input resume text using the same TF-IDF vectorizer\n",
        "    input_resume_vector = tfidf_vectorizer.transform([input_resume_text])\n",
        "\n",
        "    # Calculated cosine similarity between the input resume and all resumes in the database\n",
        "    cosine_similarities = calculate_cosine_similarity(input_resume_vector, tfidf_matrix)\n",
        "\n",
        "    # Get the indices of the most similar resumes\n",
        "    indices = cosine_similarities.argsort()[0][::-1]\n",
        "\n",
        "    # Print the top 5 most similar resume filenames and their similarity scores\n",
        "    top_5_similar_resumes = [(list(json_data.keys())[i], cosine_similarities[0][i]) for i in indices[:5]]\n",
        "    print(\"Top 5 most similar resume filenames and their similarity scores:\")\n",
        "    for filename, similarity_score in top_5_similar_resumes:\n",
        "        print(f\"Filename: {filename}, Similarity Score: {similarity_score:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2 Challenges**\n",
        "**1.Reducing Resume Processing Time Efficiently:**\n",
        "- In this case I have used all resumes from Agriculture and Arts Resume Database. At first, it took about 1 hour to process them all, which was quite slow.\n",
        "- To overcome this challenge and boost efficiency, I implemented advanced parallel processing techniques, specifically leveraging Python's multiprocessing capabilities. By this method, I was able to significantly reduce the processing time to just 30 minutes.\n",
        "\n",
        "**2.Overcoming Token Limitations with Data Chunking:**\n",
        "\n",
        "- One of the key obstacles encountered when working with GPT-3 was its token limitation, which capped the input at 4097 tokens. This posed a limitation when dealing with longer resume texts that exceeded this threshold.\n",
        "- To overcome this challenge and ensure that the GPT-3 model could effectively process the entire content of a resume, I employed a data chunking strategy. Instead of sending the entire resume at once, I divided the text into manageable chunks.\n",
        "- These chunks were then sequentially processed by the model, allowing it to read and enhance all the data within the resume. This approach ensured that no vital information was omitted and that the enhanced output was comprehensive and accurate.\n",
        "\n",
        "**3.Data Preprocessing:**\n",
        "- To optimize the use of the limited tokens available, I employed data preprocessing techniques before sending the text to GPT-3. This preprocessing step involved carefully preparing and structuring the data to extract only the essential information. By doing so, we ensured that GPT-3 could comprehend and provide meaningful responses, even within the token constraints. This approach not only allowed us to work efficiently with token limitations but also enhanced the overall quality of the responses generated by GPT-3."
      ],
      "metadata": {
        "id": "LIu36GdOk1gB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 2 Future Enhancements**\n",
        "\n",
        "**Processing Time:** Leveraging AWS' formidable cloud computing capabilities to introduce advanced parallel processing holds the promise of a faster and more efficient resume processing pipeline. By capitalizing on the cloud's scalability and abundant resources, we can execute numerous CPU-intensive tasks concurrently. This not only unlocks the potential for substantial performance improvements but also offers the possibility of cost reduction by minimizing reliance on on-premises hardware. Ultimately, this enhancement will usher in an era of enhanced efficiency and cost-effectiveness, further elevating the overall effectiveness of our system."
      ],
      "metadata": {
        "id": "xifwYKFPk9PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deliverables PART 2:**\n",
        "- Task 1:\n",
        "Input_data_Task_1,\n",
        "Output_task_1\n",
        "- Task 2:\n",
        "Input_data_Task_2,\n",
        "Output_task_2\n"
      ],
      "metadata": {
        "id": "KxStWDFvuuWY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}